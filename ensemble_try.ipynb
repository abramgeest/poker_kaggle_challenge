{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB,BernoulliNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a large set of known poker hands, train a machine learning model to identify a poker hand (rules guessing, is pretty hard for machine learning, but an interesting excercise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE:  I did this project after the kaggle competition finished.  So I don't officially have anything to submit/compare to.  Therefore I create a rules based model to compare to.  I then give what my kaggle score would have been based on these rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The values of the hands are:</p>\n",
    "<p>\n",
    "0: Nothing in hand; not a recognized poker hand <br>\n",
    "1: One pair; one pair of equal ranks within five cards<br>\n",
    "2: Two pairs; two pairs of equal ranks within five cards<br>\n",
    "3: Three of a kind; three equal ranks within five cards<br>\n",
    "4: Straight; five cards, sequentially ranked with no gaps<br>\n",
    "5: Flush; five cards with the same suit<br>\n",
    "6: Full house; pair + different rank three of a kind<br>\n",
    "7: Four of a kind; four equal ranks within five cards<br>\n",
    "8: Straight flush; straight + flush<br>\n",
    "9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data and setting up df with train/test split of 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>C1</th>\n",
       "      <th>S2</th>\n",
       "      <th>C2</th>\n",
       "      <th>S3</th>\n",
       "      <th>C3</th>\n",
       "      <th>S4</th>\n",
       "      <th>C4</th>\n",
       "      <th>S5</th>\n",
       "      <th>C5</th>\n",
       "      <th>hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S1  C1  S2  C2  S3  C3  S4  C4  S5  C5  hand\n",
       "0   4   9   2   1   2   2   4   7   2   8     0\n",
       "1   1   4   3   6   1  12   3  11   2   7     0\n",
       "2   1  11   4   1   3   7   4  11   2   1     2\n",
       "3   2   9   2   4   3   6   1   9   4   9     3\n",
       "4   1   8   2   4   2  11   2   2   2   1     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainfile=\"train.csv\"\n",
    "fulldf=pd.read_csv(trainfile)\n",
    "\n",
    "fulldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df,dftest=train_test_split(fulldf,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a function that determines the hand from pre-defined rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#also used in feature construction\n",
    "def f(a):\n",
    "    return np.max(np.bincount(np.array(a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rules(h):\n",
    "    s=h[5:] #suits\n",
    "    c=np.sort(h[:5]) # card values\n",
    "    fl=f(s)==5 # is flush t/f\n",
    "    diff=np.diff(c) #taking the difference between sorted card values\n",
    "    sets=np.bincount(np.bincount(c),minlength=5)[1:] # counting the number of pairs and sets\n",
    "    rst=strait=False  # pre conditioning the strait and royal strait booleans\n",
    "\n",
    "    #print c\n",
    "    #determining if a strait or royal strait exisits\n",
    "    if np.bincount(diff)[1]==4:\n",
    "        strait=True\n",
    "    elif all([a==b for a,b in zip(diff,[9,1,1,1])]):\n",
    "        rst=True\n",
    "    \n",
    "    #assigning strait, flush, strait flush, and royal strait flush\n",
    "    if (rst and not fl) or (strait and not fl):\n",
    "        return 4\n",
    "    elif fl and not rst and not strait:\n",
    "        return 5\n",
    "    elif strait and fl:\n",
    "        return 8\n",
    "    elif rst and fl:\n",
    "        return 9\n",
    "    \n",
    "    if sets[1]==1 and sets[2]==1:\n",
    "        return 6\n",
    "    elif sets[1]==1:\n",
    "        return 1\n",
    "    elif sets[1]==2:\n",
    "        return 2\n",
    "    elif sets[2]==1:\n",
    "        return 3\n",
    "    elif sets[4-1]==1:\n",
    "        return 7\n",
    "    \n",
    "    #elif (not fl) and sets[0]==5 and not rst and not strait:\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I am double checking that the rules correctly label all the training data.  -- It does.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25010 25010 0.0\n"
     ]
    }
   ],
   "source": [
    "fulldf['ruleout']=map(lambda x1,x2,x3,x4,x5,y1,y2,y3,y4,y5:rules([x1,x2,x3,x4,x5,y1,y2,y3,y4,y5]),\n",
    "    fulldf['C1'],fulldf['C2'],fulldf['C3'],fulldf['C4'],fulldf['C5'],\n",
    "    fulldf['S1'],fulldf['S2'],fulldf['S3'],fulldf['S4'],fulldf['S5']\n",
    "    )\n",
    "fulldf['outdiff']=map(lambda a,b:np.fabs(a-b),fulldf['ruleout'],fulldf['hand'])\n",
    "\n",
    "sumh= sum([a==b for a,b in zip(fulldf['ruleout'],fulldf['hand'])])\n",
    "lh= len(fulldf['hand'])\n",
    "print lh,sumh,1.*(lh-sumh)/lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up features and prediction analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def features_gen(df):\n",
    "    '''The input data frame should not have \"hand\" '''\n",
    "    dfout=df.copy(deep=True)\n",
    "\n",
    "    #Sort the cards into order\n",
    "    dfout['cards'] = map(lambda x1,x2,x3,x4,x5:np.sort([x1,x2,x3,x4,x5]),df['C1'],df['C2'],df['C3'],df['C4'],df['C5'])\n",
    "    for i in xrange(0,5):\n",
    "        s=\"C{:1d}\".format(i+1)\n",
    "        dfout[s]=map(lambda x1:x1[i],dfout['cards'])\n",
    "\n",
    "    #Sort the suits into order\n",
    "    dfout['suits'] = map(lambda x1,x2,x3,x4,x5:np.sort([x1,x2,x3,x4,x5]),df['S1'],df['S2'],df['S3'],df['S4'],df['S5'])\n",
    "    for i in xrange(0,5):\n",
    "        s=\"S{:1d}\".format(i+1)\n",
    "        dfout[s]=map(lambda x1:x1[i],dfout['suits'])\n",
    "    \n",
    "    #get the difference between the sorted card values\n",
    "    dfout['diff'] = map(lambda x1,x2,x3,x4,x5:np.diff(np.sort([x1,x2,x3,x4,x5,x1+13])),df['C1'],df['C2'],df['C3'],df['C4'],df['C5'])\n",
    "    for i in xrange(0,5):\n",
    "        s=\"D{:1d}\".format(i+1)\n",
    "        dfout[s]=map(lambda x1:x1[i],dfout['diff'])\n",
    "    \n",
    "    #the number of cards of the suit with the most cards (5 is a flush)\n",
    "    dfout['flush'] = map(lambda x1,x2,x3,x4,x5:f([x1,x2,x3,x4,x5]),df['S1'],df['S2'],df['S3'],df['S4'],df['S5'])\n",
    "    \n",
    "    #Drop temporary fields\n",
    "    dfout=dfout.drop('suits',1)\n",
    "    dfout=dfout.drop('cards',1)\n",
    "    dfout=dfout.drop('diff',1)\n",
    "    \n",
    "    return dfout[['S1','S2','S3','S4','S5','C1','C2','C3','C4','C5','D1','D2','D3','D4','D5','flush']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyzing_preditions(Xtest):\n",
    "    s=svc.predict(Xtest)\n",
    "    r=rfc.predict(Xtest)\n",
    "    l=lrc.predict(Xtest)\n",
    "    n=nb.predict(Xtest)\n",
    "\n",
    "    print \"Preditctions without feature Engineering (sorted card numbers)\"\n",
    "    string=\"{:14s} {:7.6f}%\".format(\"SVM acc.:\",svc.score(Xtest,ytest))\n",
    "    print string\n",
    "    string=\"{:14s} {:7.6f}%\".format(\"RFC acc.:\",rfc.score(Xtest,ytest))\n",
    "    print string\n",
    "    string=\"{:14s} {:7.6f}%\".format(\"Logit R. acc.:\", lrc.score(Xtest,ytest))\n",
    "    print string\n",
    "    string=\"{:14s} {:7.6f}%\".format(\"Naive B. acc.:\", nb.score(Xtest,ytest))\n",
    "    print string\n",
    "\n",
    "    out=[max([a,b]) for a,b,c in zip(s,r,n)]\n",
    "    su=sum([a==b for a,b in zip(out,ytest)])\n",
    "    la=len(out)\n",
    "    string=\"{:14s} {:7.6f}%  (max hand of rfc,svm,nb)\".format(\"Ensemble acc.:\", 1.*su/la)\n",
    "    print string\n",
    "\n",
    "\n",
    "    out=[np.argmax(np.bincount(np.array([a,b,c,d]))) for a,b,c,d in zip(s,r,n,l)]\n",
    "    su=sum([a==b for a,b in zip(out,ytest)])\n",
    "    la=len(out)\n",
    "    string=\"{:14s} {:7.6f}%  (voting from rfc,svm,nb)\".format(\"Ensemble acc.:\", 1.*su/la)\n",
    "    print string\n",
    "    \n",
    "    print \"----------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models with different features as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I use the same models (Random Forest (rfc/rf), SVM, logistic regression (lgc), and Bernoulli Naise Baise with 2 ensembles) to get results.  I then vary the features used from directly as inputted to heavily processed features.  The final features are the number of cards of suit with the most cards, and the difference between the sorted card values.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the data as given to fit the models - no sorting or anything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[['S1','S2','S3','S4','S5','C1','C2','C3','C4','C5']]\n",
    "y=df.hand\n",
    "\n",
    "svc = svm.SVC()#decision_function_shape='ovo')\n",
    "svc.fit(X,y)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X,y)\n",
    "\n",
    "lrc=LogisticRegression(C=1.)\n",
    "lrc.fit(X,y)\n",
    "\n",
    "nb = BernoulliNB() \n",
    "nb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preditctions without feature Engineering (sorted card numbers)\n",
      "SVM acc.:      0.582767%\n",
      "RFC acc.:      0.600560%\n",
      "Logit R. acc.: 0.506597%\n",
      "Naive B. acc.: 0.506597%\n",
      "Ensemble acc.: 0.589764%  (max hand of rfc,svm,nb)\n",
      "Ensemble acc.: 0.506597%  (voting from rfc,svm,nb)\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "#pro=features_gen(dftest.drop('hand',1))\n",
    "Xtest=dftest[['S1','S2','S3','S4','S5','C1','C2','C3','C4','C5']]\n",
    "ytest=dftest.hand\n",
    "\n",
    "analyzing_preditions(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just using the sorted Card numbers and sorted Suits to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed=features_gen(df.drop('hand',1))\n",
    "X=processed[['S1','S2','S3','S4','S5','C1','C2','C3','C4','C5']]\n",
    "y=df.hand\n",
    "\n",
    "svc = svm.SVC()#decision_function_shape='ovo')\n",
    "svc.fit(X,y)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X,y)\n",
    "\n",
    "lrc=LogisticRegression(C=1.)\n",
    "lrc.fit(X,y)\n",
    "\n",
    "nb = BernoulliNB() \n",
    "nb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preditctions without feature Engineering (sorted card numbers)\n",
      "SVM acc.:      0.951020%\n",
      "RFC acc.:      0.944622%\n",
      "Logit R. acc.: 0.549380%\n",
      "Naive B. acc.: 0.506597%\n",
      "Ensemble acc.: 0.963015%  (max hand of rfc,svm,nb)\n",
      "Ensemble acc.: 0.692923%  (voting from rfc,svm,nb)\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "pro=features_gen(dftest.drop('hand',1))\n",
    "Xtest=pro[['S1','S2','S3','S4','S5','C1','C2','C3','C4','C5']]\n",
    "ytest=dftest.hand\n",
    "\n",
    "analyzing_preditions(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Feature Engineering (Features are: the differences between sorted card values, and the number of cards of the most numerous suit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed=features_gen(df.drop('hand',1))\n",
    "#X=processed[['S1','S2','S3','S4','S5','D1','D2','D3','D4','D5','C1','C2','C3','C4','C5','flush']]\n",
    "X=processed[['D1','D2','D3','D4','D5','flush']]\n",
    "y=df.hand\n",
    "\n",
    "svc = svm.SVC()#decision_function_shape='ovo')\n",
    "svc.fit(X,y)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X,y)\n",
    "\n",
    "lrc=LogisticRegression(C=1.)\n",
    "lrc.fit(X,y)\n",
    "\n",
    "nb = BernoulliNB() \n",
    "nb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preditctions without feature Engineering (sorted card numbers)\n",
      "SVM acc.:      0.989204%\n",
      "RFC acc.:      1.000000%\n",
      "Logit R. acc.: 0.552779%\n",
      "Naive B. acc.: 0.918233%\n",
      "Ensemble acc.: 0.999400%  (max hand of rfc,svm,nb)\n",
      "Ensemble acc.: 0.943423%  (voting from rfc,svm,nb)\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "pro=features_gen(dftest.drop('hand',1))\n",
    "#Xtest=pro[['S1','S2','S3','S4','S5','D1','D2','D3','D4','D5','C1','C2','C3','C4','C5','flush']]\n",
    "Xtest=pro[['D1','D2','D3','D4','D5','flush']]\n",
    "ytest=dftest.hand\n",
    "\n",
    "analyzing_preditions(Xtest)\n",
    "\n",
    "final_model=rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion before I \"submit\" my answers to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<b>Result:</b> So with the proper features selected we see that the Random Forest gives a perfect result.  As such lets select this as our final model to be \"submitted to kaggle\" (really compared to a set of rules I wrote).  However, as a data scientist I find it interesting to consider the various models and features:<br>\n",
    "\n",
    "<b>Progression of Features:</b> So we see that the more I mess with the features the better the accuracy gets particularly with the SVM and random forest models since I am effectively providing more and more information to the model with each improvement of the features.   <br>\n",
    "\n",
    "<b> Models:</b><br>\n",
    "<u>Random Forest:</u> Makes sense it does the best of the non-emsemble methods because a decision tree can almost be consided auto generated rules.  However, it is worth noting that for some runs the random forest is beaten by an ensemble including the Random Forest and the SVM.<br>\n",
    "<u>SVM:</u>  Does pretty well because support vector machines are a non linear model that draws hyper-planes around the various catagories<br>\n",
    "<u>LogisticRegression:</u> Generally does not do well regardless of the features because it is a linear method the catagories are deffinately non-linear.<br>\n",
    "<u>Bernoulli Naive Baise:</u> Does fairly wellwith compilcated features but generally the known problems of Naie Baise make it not as good as the SVM/RF.<br>\n",
    "<u> Ensemble of RF and SVM:</u>  Here the highest hand is selected from the two.  Generally, this is the highest or second highest model.  With less processed features this model tends to do best, but is beaten by RF with higher processed features.<br>\n",
    "<u> Ensemble of all models voting: </u>  This model does fairly well but is poisoned by the logistic regression model and NB models.<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:  I am including the rules output for the test.csv File because the answers aren't included now that the kaggle challenge is closed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating an output file for the test data and the rules data - \"Submitting\" my answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fi=\"test.csv\"\n",
    "testdf=pd.read_csv(fi)\n",
    "featdf=features_gen(testdf.drop('id',1))\n",
    "X=featdf[['D1','D2','D3','D4','D5','flush']]\n",
    "#featdf['hand']=final_model.predict(X)\n",
    "#out=pd.DataFrame(featdf['hand'],index=testdf['id'])\n",
    "out=pd.DataFrame()\n",
    "out['id']=testdf['id']\n",
    "out['hand']=final_model.predict(X)\n",
    "out['ruleout']=map(lambda x1,x2,x3,x4,x5,y1,y2,y3,y4,y5:rules([x1,x2,x3,x4,x5,y1,y2,y3,y4,y5]),\n",
    "    testdf['C1'],testdf['C2'],testdf['C3'],testdf['C4'],testdf['C5'],\n",
    "    testdf['S1'],testdf['S2'],testdf['S3'],testdf['S4'],testdf['S5']\n",
    "    )\n",
    "\n",
    "fileout=\"/Users/abramvandergeest/fun+progs/poker2/test_predict.csv\"\n",
    "out[['id','hand']].to_csv(fileout,index=False)\n",
    "fileout=\"/Users/abramvandergeest/fun+progs/poker2/test_rules.csv\"\n",
    "out[['id','ruleout']].to_csv(fileout,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So below is the comparison between my predicted results and the rules (or what kaggle would have had).  We see that I would have gotten a kaggle score of (1-2.e-5)=0.99998.  So pretty good without fiddeling to get the best test data result (WHICH WOULD BE BAD! since you are FITTING TO THE TEST DATA).  The exact comparison depends on the \"randomized fitting\" so the comparison varies a bit with each run.  We see that the biggest error is that the 4 of a kind is most often considered a three of a kind.  Occasionally, a Royal strait flush is only considered a flush.  Both of these errors makes sense since they are the rarest examples in the training set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  S1  C1  S2  C2  S3  C3  S4  C4  S5  C5  ruleout  hand  diff\n",
      "54180    54181   3   1   1   5   3   5   4   5   2   5        7     3     4\n",
      "94010    94011   2   2   1   4   3   4   2   4   4   4        7     3     4\n",
      "100795  100796   3   2   2  11   4  11   3  11   1  11        7     3     4\n",
      "178405  178406   3   3   2  10   4  10   3  10   1  10        7     3     4\n",
      "193692  193693   4   3   4   6   2   6   3   6   1   6        7     3     4\n",
      "198727  198728   4   4   1   8   3   8   4   8   2   8        7     3     4\n",
      "325202  325203   2   8   2   9   3   9   4   9   1   9        7     3     4\n",
      "440176  440177   1   3   3  12   4  12   2  12   1  12        7     3     4\n",
      "467572  467573   4   1   3   3   4   3   1   3   2   3        7     3     4\n",
      "531760  531761   2   2   4   6   1   6   3   6   2   6        7     3     4\n",
      "587698  587699   2   6   1   9   2   9   3   9   4   9        7     3     4\n",
      "611320  611321   2   2   3   9   4   9   2   9   1   9        7     3     4\n",
      "635356  635357   1   4   4   5   1   5   3   5   2   5        7     3     4\n",
      "732598  732599   1   5   1   9   4   9   3   9   2   9        7     3     4\n",
      "804707  804708   3   1   1   5   2   5   4   5   3   5        7     3     4\n",
      "852081  852082   1   1   1   9   2   9   3   9   4   9        7     3     4\n",
      "897915  897916   2   3   4   5   2   5   3   5   1   5        7     3     4\n",
      "907407  907408   2   4   3   8   2   8   4   8   1   8        7     3     4\n",
      "912527  912528   3   1   2   6   3   6   1   6   4   6        7     3     4\n",
      "916664  916665   4   9   3  13   2  13   1  13   4  13        7     3     4\n"
     ]
    }
   ],
   "source": [
    "out['ruleout']=map(lambda x1,x2,x3,x4,x5,y1,y2,y3,y4,y5:rules([x1,x2,x3,x4,x5,y1,y2,y3,y4,y5]),\n",
    "    testdf['C1'],testdf['C2'],testdf['C3'],testdf['C4'],testdf['C5'],\n",
    "    testdf['S1'],testdf['S2'],testdf['S3'],testdf['S4'],testdf['S5']\n",
    "    )\n",
    "\n",
    "testdf['ruleout']=out['ruleout']\n",
    "testdf['hand']=out['hand']\n",
    "testdf['diff']=map(lambda a,b:np.fabs(a-b),testdf['ruleout'],testdf['hand'])\n",
    "\n",
    "print testdf[testdf['diff']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 999980 2e-05 0.99998\n"
     ]
    }
   ],
   "source": [
    "sumh= sum([a==b for a,b in zip(out['ruleout'],out['hand'])])\n",
    "lh= len(out['hand'])\n",
    "print lh,sumh,1.*(lh-sumh)/lh,1.-1.*(lh-sumh)/lh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
